{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6122edeb-565b-498e-b42a-a923b84ce3d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb565163-0077-43df-be22-3942b0705e29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import json\n",
    "import csv\n",
    "import requests\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import zscore\n",
    "from sqlalchemy.exc import OperationalError \n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.exc import ProgrammingError\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String,text\n",
    "import mysql.connector\n",
    "import time\n",
    "import logging\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mysql.connector\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a6bdad-54f4-4310-96ea-b05191edfa47",
   "metadata": {},
   "source": [
    "# Load unseen data in mysql localhost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cdf95635-cc91-4120-8852-0ef4b5e5556e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful\n"
     ]
    }
   ],
   "source": [
    "# Replace 'mysql' with 'mysql+mysqldb'\n",
    "engine = create_engine('mysql+mysqldb://root:password@localhost/sakila?charset=utf8mb4&binary_prefix=true')\n",
    "try:\n",
    "    connection = engine.connect()\n",
    "    print(\"Connection successful\")\n",
    "except OperationalError as e:\n",
    "    print(f\"Something went wrong: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "889da98f-5db4-4a74-ac18-e4dcd08d04b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred: (MySQLdb.ProgrammingError) (1007, \"Can't create database 'pipline'; database exists\")\n",
      "[SQL: CREATE DATABASE pipline]\n",
      "(Background on this error at: https://sqlalche.me/e/14/f405)\n"
     ]
    }
   ],
   "source": [
    "db_url = \"mysql+mysqldb://root:password@localhost/sakila\"\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "database_name = \"pipline\"\n",
    "try:\n",
    "    # Creating a connection from the engine\n",
    "    connection = engine.connect()\n",
    "\n",
    "    # Executing the query on the connection using text\n",
    "    create_database_query = text(f\"CREATE DATABASE {database_name}\")\n",
    "    connection.execute(create_database_query)\n",
    "\n",
    "    print(f\"Database {database_name} created successfully!\")\n",
    "\n",
    "    # Closing the connection after use\n",
    "    connection.close()\n",
    "\n",
    "except ProgrammingError as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "#creat a new database by use (execute)  and ensure the creation using try and exept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5a25992f-8b73-426b-a67e-8b2b7919fc28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "engine = create_engine('mysql+mysqldb://root:password@localhost/pipline?charset=utf8mb4', pool_pre_ping=True)\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shath\\\\Downloads\\\\intrusion_unseen.csv\")\n",
    "df = df.sort_index(axis=1)\n",
    "df.drop('Timestamp',axis = 1,inplace=True)\n",
    "numerical_cols_encoded = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "df[numerical_cols_encoded].empty\n",
    "# Standardize numerical features to have mean=0 and variance=1\n",
    "scaler = StandardScaler()\n",
    "df[numerical_cols_encoded] = scaler.fit_transform(df[numerical_cols_encoded])\n",
    "\n",
    "try:\n",
    "    df.to_sql('second', con=engine, index=False, if_exists='replace')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6536225f-a02d-4de2-846a-fe64468ba890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cd45179-e8b9-454f-9e87-522fbb674ef4",
   "metadata": {},
   "source": [
    "# 1. Data Extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8b7ce68a-afea-4c9f-a140-36a8f8028c5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "# step 1 : extract data \n",
    "def extract_data():\n",
    "    api_endpoint = 'http://87.236.232.200:5000/data'\n",
    "    # Make the API request and get the JSON response\n",
    "    response = requests.get(api_endpoint)\n",
    "    data = response.json()\n",
    "    df = pd.DataFrame(data)\n",
    "    df.drop('source_ip',axis = 1,inplace=True)\n",
    "    df.drop('timestamp',axis = 1,inplace=True)\n",
    "    df.drop('Timestamp',axis = 1,inplace=True)\n",
    "    for column in df.columns:\n",
    "        if column != \"Label\":\n",
    "            df[column] = df[column].astype('float64')\n",
    "    logging.info(\"Extracting data...\")\n",
    "    time.sleep(2)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b7a41f-5909-4a46-8bf0-bc3e0125170b",
   "metadata": {},
   "source": [
    "# 2. Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "82398ce0-4fa6-4a25-9c55-a8cfc2bf7cd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_data(df):\n",
    "    # Task 1: Handling Missing Values\n",
    "    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    imputer_numerical = SimpleImputer(strategy='mean')\n",
    "    imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
    "    df[numerical_cols] = imputer_numerical.fit_transform(df[numerical_cols])\n",
    "    df[categorical_cols] = imputer_categorical.fit_transform(df[categorical_cols])\n",
    "\n",
    "    # Detection of multivariate outliers\n",
    "    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    data = df[numerical_cols].values  # Convert DataFrame to NumPy array\n",
    "\n",
    "    # Calculate mean and covariance matrix\n",
    "    mean_vector = np.mean(data, axis=0)\n",
    "    cov_matrix = np.cov(data, rowvar=False)\n",
    "\n",
    "    # Add regularization term to make the covariance matrix invertible\n",
    "    reg_term = 1e-6  # Adjust if needed\n",
    "    cov_matrix += reg_term * np.identity(cov_matrix.shape[0])\n",
    "\n",
    "    try:\n",
    "        # Calculate Mahalanobis distance\n",
    "        mahalanobis_distances = np.array([np.dot(np.dot((x - mean_vector), np.linalg.inv(cov_matrix)), (x - mean_vector)) for x in data])\n",
    "\n",
    "        # Calculate z-scores\n",
    "        mean = np.mean(mahalanobis_distances)\n",
    "        std_dev = np.std(mahalanobis_distances)\n",
    "        z_scores = np.abs((mahalanobis_distances - mean) / std_dev)\n",
    "\n",
    "        # Step 3: Determine the threshold\n",
    "        alpha = 0.05\n",
    "        n = len(mahalanobis_distances)\n",
    "        critical_value = t.ppf(1 - alpha / (2 * n), df=n - 2)\n",
    "        threshold = (n - 1) / np.sqrt(n) * (critical_value / np.sqrt(n - 2 + critical_value**2))\n",
    "\n",
    "        # Step 4: Identify outliers\n",
    "        outliers = df[z_scores >= threshold]\n",
    "\n",
    "        print(f\"Mean Vector: {mean_vector}\")\n",
    "        print(f\"Covariance Matrix:\\n{cov_matrix}\")\n",
    "        print(f\"Outliers:\\n{outliers}\")\n",
    "\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"Singular matrix encountered. Adjust regularization term or handle collinearity.\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Task 2: Filtering Outliers\n",
    "    z_scores = np.abs(zscore(df[numerical_cols]))\n",
    "    threshold = 3\n",
    "    df_no_outliers = df[(z_scores < threshold).all(axis=1)]\n",
    "    \n",
    "    # Task 3: Remove Duplicate Records\n",
    "    df_no_duplicates = df_no_outliers.drop_duplicates()\n",
    "    \n",
    "    # Task 4: Scaling Numerical Features\n",
    "    \n",
    "    if not df.empty:\n",
    "        # Check if there are samples in the DataFrame before scaling\n",
    "        numerical_cols_encoded = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "        if not df[numerical_cols_encoded].empty:\n",
    "            # Standardize numerical features to have mean=0 and variance=1\n",
    "            scaler = StandardScaler()\n",
    "            df[numerical_cols_encoded] = scaler.fit_transform(df[numerical_cols_encoded])\n",
    "\n",
    "        else:\n",
    "            print(\"After encoding categorical variables, there are no numerical samples to scale.\")\n",
    "    else:\n",
    "        print(\"After cleaning and filtering, the DataFrame is empty.\")\n",
    "        \n",
    "    logging.info(\"Processing data...\")\n",
    "    time.sleep(6)  # Simulating processing process\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a00c6f1-9452-414d-8f9f-2dc920fba97f",
   "metadata": {},
   "source": [
    "# 3. Machine Learning Model Integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cab7fae2-b39d-4b6e-aefb-938897b04426",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def machin_model(df):\n",
    "    X = df.drop('Label', axis=1) \n",
    "    y = df['Label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Choose a classifier (e.g., RandomForestClassifier)\n",
    "    classifier = RandomForestClassifier()\n",
    "    \n",
    "    # Train the model\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions on the test set\n",
    "    y_pred = classifier.predict(X_test)\n",
    "     \n",
    "    # Evaluate model performance\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    classification_rep = classification_report(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_rep)\n",
    "    \n",
    "    # Connect to MySQL using SQLAlchemy\n",
    "    conn = mysql.connector.connect(user='root', password='password', host='localhost', database='pipline')\n",
    "    engine = create_engine('mysql+mysqlconnector://root:password@localhost/pipline')\n",
    "    \n",
    "    # Load unseen data\n",
    "    query = \"SELECT * FROM second\"\n",
    "    df_unseen = pd.read_sql(query, engine)\n",
    "    \n",
    "    # Close MySQL connection\n",
    "    conn.close()\n",
    "        \n",
    "    # Features for the unseen data\n",
    "    X_unseen = df_unseen.drop('Label', axis=1)\n",
    "    \n",
    "    # Apply the trained model to predict labels for the unseen data\n",
    "    y_pred_unseen = classifier.predict(X_unseen)\n",
    "    \n",
    "    logging.info(\"Machine Model...\")\n",
    "    time.sleep(6)  # Simulating processing process\n",
    "    \n",
    "    # Display predictions for the unseen data\n",
    "    print(\"Predictions for Unseen Data:\")\n",
    "    print(y_pred_unseen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baf58c9-a433-4578-9b83-b4acd725935e",
   "metadata": {},
   "source": [
    "# 4. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dd08c883-1271-4a5e-8fbf-3ae27787f089",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(df):\n",
    "    engine = create_engine('mysql+mysqldb://root:password@localhost/pipline')\n",
    "    df.to_sql('transformed', con=engine, index=False, if_exists='replace')\n",
    "    logging.info(\"Loading data...\")\n",
    "    time.sleep(2)  # Simulating loading process\n",
    "    logging.info(\"Data loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "443fd8e1-cd6c-47da-bc0f-394b9eecebe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-10 13:38:56,914 - INFO - Extracting data...\n",
      "2024-01-10 13:38:59,105 - INFO - Processing data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Vector: [8.49000000e-01 1.19585000e+02 1.19585000e+02 1.19585000e+02\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.76960000e+01\n",
      " 7.04586738e+05 2.09575884e+05 2.42294600e+03 3.74955730e+05\n",
      " 7.18732454e+05 0.00000000e+00 1.79688000e+02 5.81574833e+01\n",
      " 0.00000000e+00 9.90362039e+01 0.00000000e+00 4.72351562e+04\n",
      " 5.81574833e+01 0.00000000e+00 0.00000000e+00 2.02000000e-01\n",
      " 2.09520610e+04 0.00000000e+00 0.00000000e+00 1.02167883e+03\n",
      " 2.53317590e+06 2.45486719e+06 3.42757070e+05 5.92003200e+03\n",
      " 8.90770683e+05 9.49543428e+04 4.71000000e-01 0.00000000e+00\n",
      " 0.00000000e+00 1.09216000e+02 2.47520810e+06 6.56200438e+05\n",
      " 6.79467400e+03 1.24334685e+06 2.53208645e+06 0.00000000e+00\n",
      " 4.19919000e+02 8.59929405e+01 0.00000000e+00 1.89754953e+02\n",
      " 0.00000000e+00 4.77191866e+04 8.59929405e+01 3.30480000e+01\n",
      " 0.00000000e+00 8.26802793e+05 8.26802793e+05 8.26802793e+05\n",
      " 0.00000000e+00 9.97710000e+01 4.25121600e+03 1.51000000e-01\n",
      " 4.37580000e+02 6.70885851e+01 0.00000000e+00 1.47580839e+02\n",
      " 4.65753056e+04 7.52462521e+01 6.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 2.01028000e+02 1.73000000e+00 4.20987000e+02\n",
      " 3.26300000e+00 1.73000000e+00 3.26300000e+00 2.01028000e+02\n",
      " 4.20987000e+02 2.80000000e-02]\n",
      "Covariance Matrix:\n",
      "[[1.28328327e-01 1.80754104e+01 1.80754104e+01 ... 1.16668949e+01\n",
      "  5.66797167e+01 4.23223223e-03]\n",
      " [1.80754104e+01 3.47608973e+06 3.47608973e+06 ... 2.63145562e+04\n",
      "  5.28557924e+04 1.09872492e+02]\n",
      " [1.80754104e+01 3.47608973e+06 3.47608973e+06 ... 2.63145562e+04\n",
      "  5.28557924e+04 1.09872492e+02]\n",
      " ...\n",
      " [1.16668949e+01 2.63145562e+04 2.63145562e+04 ... 6.34395728e+04\n",
      "  8.79044548e+04 9.60081682e+00]\n",
      " [5.66797167e+01 5.28557924e+04 5.28557924e+04 ... 8.79044548e+04\n",
      "  2.10916475e+05 6.75511912e+00]\n",
      " [4.23223223e-03 1.09872492e+02 1.09872492e+02 ... 9.60081682e+00\n",
      "  6.75511912e+00 2.72442432e-02]]\n",
      "Outliers:\n",
      "     ACK Flag Cnt  Active Max  Active Mean  Active Min  Active Std  \\\n",
      "92            1.0     41003.0      41003.0     41003.0         0.0   \n",
      "200           1.0     33115.0      33115.0     33115.0         0.0   \n",
      "797           1.0       319.0        319.0       319.0         0.0   \n",
      "799           1.0       263.0        263.0       263.0         0.0   \n",
      "897           1.0     21480.0      21480.0     21480.0         0.0   \n",
      "\n",
      "     Bwd Blk Rate Avg  Bwd Byts/b Avg  Bwd Header Len  Bwd IAT Max  \\\n",
      "92                0.0             0.0           136.0   64500000.0   \n",
      "200               0.0             0.0           136.0   64800000.0   \n",
      "797               0.0             0.0           136.0   65000000.0   \n",
      "799               0.0             0.0           200.0   65400000.0   \n",
      "897               0.0             0.0           200.0   64800000.0   \n",
      "\n",
      "     Bwd IAT Mean  ...  SYN Flag Cnt  Subflow Bwd Byts  Subflow Bwd Pkts  \\\n",
      "92     21500000.0  ...           0.0             354.0               4.0   \n",
      "200    21600000.0  ...           0.0             323.0               4.0   \n",
      "797    21700000.0  ...           0.0             360.0               4.0   \n",
      "799    13100000.0  ...           0.0             380.0               6.0   \n",
      "897    13000000.0  ...           0.0             347.0               6.0   \n",
      "\n",
      "     Subflow Fwd Byts  Subflow Fwd Pkts  Tot Bwd Pkts  Tot Fwd Pkts  \\\n",
      "92              935.0               5.0           4.0           5.0   \n",
      "200             935.0               5.0           4.0           5.0   \n",
      "797             935.0               5.0           4.0           5.0   \n",
      "799             935.0               4.0           6.0           4.0   \n",
      "897             935.0               4.0           6.0           4.0   \n",
      "\n",
      "     TotLen Bwd Pkts  TotLen Fwd Pkts  URG Flag Cnt  \n",
      "92             354.0            935.0           1.0  \n",
      "200            323.0            935.0           1.0  \n",
      "797            360.0            935.0           1.0  \n",
      "799            380.0            935.0           1.0  \n",
      "897            347.0            935.0           1.0  \n",
      "\n",
      "[5 rows x 79 columns]\n",
      "Accuracy: 1.0\n",
      "Classification Report:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                  Benign       1.00      1.00      1.00        84\n",
      "        DoS attacks-Hulk       1.00      1.00      1.00        87\n",
      "DoS attacks-SlowHTTPTest       1.00      1.00      1.00        29\n",
      "\n",
      "                accuracy                           1.00       200\n",
      "               macro avg       1.00      1.00      1.00       200\n",
      "            weighted avg       1.00      1.00      1.00       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-10 13:39:05,557 - INFO - Machine Model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for Unseen Data:\n",
      "['DoS attacks-Hulk' 'Benign' 'DoS attacks-Hulk' 'Benign' 'Benign'\n",
      " 'DoS attacks-SlowHTTPTest' 'Benign' 'Benign' 'DoS attacks-SlowHTTPTest'\n",
      " 'Benign']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-10 13:39:11,970 - INFO - Loading data...\n",
      "2024-01-10 13:39:13,971 - INFO - Data loaded successfully\n",
      "2024-01-10 13:39:13,972 - INFO - Pipeline executed successfully\n",
      "2024-01-10 13:39:30,228 - INFO - Extracting data...\n",
      "2024-01-10 13:39:32,272 - INFO - Processing data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Classification Report:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                  Benign       1.00      1.00      1.00        94\n",
      "        DoS attacks-Hulk       1.00      1.00      1.00        86\n",
      "DoS attacks-SlowHTTPTest       1.00      1.00      1.00        20\n",
      "\n",
      "                accuracy                           1.00       200\n",
      "               macro avg       1.00      1.00      1.00       200\n",
      "            weighted avg       1.00      1.00      1.00       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-10 13:39:38,821 - INFO - Machine Model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for Unseen Data:\n",
      "['DoS attacks-Hulk' 'Benign' 'DoS attacks-Hulk' 'Benign' 'Benign'\n",
      " 'DoS attacks-SlowHTTPTest' 'Benign' 'Benign' 'DoS attacks-SlowHTTPTest'\n",
      " 'Benign']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-10 13:39:45,226 - INFO - Loading data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 28\u001b[0m\n\u001b[0;32m     24\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Start monitoring the pipeline\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m monitor_pipeline()\n",
      "Cell \u001b[1;32mIn[54], line 14\u001b[0m, in \u001b[0;36mmonitor_pipeline\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m machin_model(processed_data)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Step 4: Load data\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m load_data(processed_data)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Log successful execution\u001b[39;00m\n\u001b[0;32m     17\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline executed successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[53], line 5\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mto_sql(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformed\u001b[39m\u001b[38;5;124m'\u001b[39m, con\u001b[38;5;241m=\u001b[39mengine, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Simulating loading process\u001b[39;00m\n\u001b[0;32m      6\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData loaded successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def monitor_pipeline():\n",
    "    while True:\n",
    "        try:\n",
    "            # Step 1: Extract data\n",
    "            extracted_data = extract_data()\n",
    "\n",
    "            # Step 2: Process data\n",
    "            processed_data = process_data(extracted_data)\n",
    "            \n",
    "            # Step 3: machin model\n",
    "            \n",
    "            machin_model(processed_data)\n",
    "            # Step 4: Load data\n",
    "            load_data(processed_data)\n",
    "\n",
    "            # Log successful execution\n",
    "            logging.info(\"Pipeline executed successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Log any exceptions or errors\n",
    "            logging.error(f\"Pipeline execution failed: {str(e)}\")\n",
    "\n",
    "        # Wait for a certain interval before executing the pipeline again\n",
    "        time.sleep(10)\n",
    "\n",
    "\n",
    "# Start monitoring the pipeline\n",
    "monitor_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a41f88e-0d36-4d69-a1e8-d67e8d54f5f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdb1d5f-bcc6-4386-af04-964d9dab7d56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd223dfd-338d-400b-98a7-4b9e4d9b2ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
